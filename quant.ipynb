{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dbfe6a5-9ee5-4da9-bd0d-fdee0d481f56",
   "metadata": {},
   "source": [
    "# quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8b8ad-e5fc-40d9-b22a-af34ef8098b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.什么是量化\n",
    "量化就是将浮点数转化为定点整数（通常为`int8/uint8`)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff820fc-8884-460b-ad41-7f5239bc468b",
   "metadata": {},
   "source": [
    "#### 2.为什么要做量化 \n",
    "- 减小网络体积\n",
    "- 配合奖励整数计算的硬件\n",
    "- 加速\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f104f4-333c-4298-b430-45a556536cd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.如何做量化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cb272-f62f-414f-a05b-bdb8bb150ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "&emsp;&emsp;将范围在 $[x_{min}+ x_{max}]$ 的浮点数，映射到$[q_{min}, q_{max}]$的范围中。使用直觉思考， 将xmin对应与qmin， xmax对应于qmax， 其余的数根据其于xmin的距离除以转化步长， 最后去掉小数转化为整数。  \n",
    "    表示为公式：\n",
    "    $$ x_q= (x_f - x_{min})\\frac{q_{max} - q_{min}}{x_{max}-x_{min}}\\tag{2.1}$$\n",
    "那么：\n",
    "$$ x_f= x_q\\frac{x_{max}-x_{min}}{q_{max} - q_{min}}+x_{min}\\tag{2.2}$$\n",
    "将`2.2`转化一下形式：$$ x_f= \\frac{x_{max}-x_{min}}{q_{max} - q_{min}}(x_q + x_{min}\\frac{q_{max} - q_{min}}{x_{max}-x_{min}})\\tag{2.3}$$  \n",
    "定义$$s := \\frac{x_{max}-x_{min}}{q_{max} - q_{min}}\\tag{2.4}$$定义$$zp := round(x_{min}\\frac{q_{max} - q_{min}}{x_{max}-x_{min}})\\tag{2.5}$$\n",
    "`2.3`式写为：\n",
    "$$ x_f = s (x_q + zq_x)\\tag{2.6}$$\n",
    "由`2.4`式可得：\n",
    "$$ x_q = \\frac{x_f}{s} - zq_x \\tag{2.7}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b14a5384-9ced-46c7-a66d-b04bbaae36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2 -1.   0.   0.1]\n",
      "scale:  0.009019607843137255\n",
      "zero point:  -244.0\n",
      "[  0. 133. 244. 255.]\n",
      "[-2.20078431 -1.00117647  0.          0.09921569]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def asymmetric_quant(arr):\n",
    "    arr_min, arr_max = arr.min(), arr.max()\n",
    "    s = (arr_max - arr_min) / 255 # 2.4\n",
    "    print(\"scale: \", s)\n",
    "    zp = np.round(arr_min / s) # 2.5\n",
    "    # print(np.round(255 - arr_max /s))\n",
    "    print(\"zero point: \", zp)\n",
    "    for index, i in enumerate(arr):\n",
    "        arr[index] = np.clip(np.round(i / s) - zp, 0, 255) # 2.7\n",
    "    return s, zp\n",
    "\n",
    "def asymmetric_dequant(arr, s, zq):\n",
    "    for index, i in enumerate(arr):\n",
    "        arr[index] = s * (i + zq) # 2.6\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = np.array([-2.2, -1.0, 0.0, 0.1])\n",
    "    print(a)\n",
    "    s, zp = asymmetric_quant(a)\n",
    "    print(a)\n",
    "    asymmetric_dequant(a, s, zp)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a244d-4797-48a3-a35e-906340103f49",
   "metadata": {},
   "source": [
    "量化与反量化的关键在于，浮点数0在量化后会映射为一个稳定的整数值，即zero point，在上面的代码中，等于200。这是因为在padding的时候，我们需要0（？）。padding让我们避免了处理数组的边界。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4646e11-b3e2-4a6c-929c-1a75b8e78484",
   "metadata": {},
   "source": [
    "将量化应用在网络中有两种不同的情况， 一种是量化已经训练好的网络，加快其推理速度。另一种是在训练是做伪量化。之所以叫做伪量化是因为神经网络训练的过程中需要浮点数。  \n",
    "不管在哪种中，占据大头的都是矩阵的计算，矩阵计算可以表示为：\n",
    "$$ f_1 = \\sum_{j=1}^{N}f_2^{i,j}*f_3^{j,k} + b\\tag{2.8}$$\n",
    "由2.6知：\n",
    "$$ f = s(q- zp)$$\n",
    "则2.8可以转换为：\n",
    "$$ s_1(q_1 - zp_1) = \\sum_{j=1}^{N}s_2(q_2^{i,j} - zp_2)*s_3(q_3^{j,k} - zp_3) + s_b(q_b - zp_b)\\tag{2.9}$$\n",
    "化简：\n",
    "$$ q_1 = \\frac{s_2*s_3}{s_1}\\sum_{j=1}^{N}(q_2^{i,j} - zp_2)*(q_3^{j,k} - zp_3) + zp_1 + s_b(q_b - zp_b) \\tag{2.9}$$\n",
    "其中$\\sum_{j=1}^{N}(q_2^{i,j} - zp_2)*(q_3^{j,k} - zp_3)$可以用`int32`表示，那么通常偏置b也直接使用`int32`表示，$\\frac{s_2*s_3}{s_1}$可以表示为$M_0*2^{-n}，n>=0$的格式。  \n",
    "那么2.9式可以表示为：\n",
    "$$ q_1 =  M_0*2^{-n}*uint32 + zp_1 + uint32\\tag{1.2}$$  \n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d838e00-bde0-4669-894b-060ee7e158f3",
   "metadata": {},
   "source": [
    "### symmetric\n",
    "数据偏移过大时， 对称会将偏移数据堆在$q_{max}$位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea6f770-0696-42d2-835a-fc0715885807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8 -1.   0.   0.5]\n",
      "0.009019607843137253\n",
      "-200.0 199.56521739130434\n",
      "[ 0.  0.  0. 55.]\n",
      "[  0.  89. 200. 255.]\n",
      "[  0.  89. 200. 255.]\n",
      "[-1.80392157 -1.00117647  0.          0.49607843]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sym_quant(arr):\n",
    "    s = 255 / (arr.max() - arr.min())\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = np.clip(np.round((arr[i]) * s), 0, 255)\n",
    "\n",
    "\n",
    "def asym_quant(arr):\n",
    "    s = (arr.max() - arr.min()) /255\n",
    "    print(s)\n",
    "    arr_min = arr.min()\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = np.clip(np.round((arr[i] - arr_min) / s), 0, 255)\n",
    "    # return s\n",
    "\n",
    "def asym_quant2(arr):\n",
    "    s = 255 / (arr.max() - arr.min())\n",
    "    arr_min = arr.min()\n",
    "    zp = np.round(arr_min * s)\n",
    "    print(zp, 255 - arr.max()*s)\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = np.clip(np.round(arr[i] * s) - zp, 0, 255)\n",
    "    return s, zp\n",
    "    \n",
    "def de_quant(arr, s, zp):\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = (arr[i] + zp) / s\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a0 = np.array([-1.8, -1.0, 0.0, 0.5])\n",
    "    a1 = np.array([-1.8, -1.0, 0.0, 0.5])\n",
    "    a2 = np.array([-1.8, -1.0, 0.0, 0.5])\n",
    " \n",
    "    # a = np.linspace(100, 110, 11)\n",
    "    # b = np.linspace(100, 110, 11)\n",
    "    # b1 = np.linspace(100, 110, 11)\n",
    "    print(a0)\n",
    "    sym_quant(a0)\n",
    "    asym_quant(a1)\n",
    "    s, zp = asym_quant2(a2)\n",
    "    print(a0)\n",
    "    print(a1)\n",
    "    print(a2)\n",
    "    de_quant(a2, s, zp)\n",
    "    print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c9ef9-b000-4eed-97fd-c64238dbe856",
   "metadata": {},
   "source": [
    "#### asymmetrc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece84615-eec8-47ba-9236-86c511b6d3b6",
   "metadata": {},
   "source": [
    "一般而言，无论per channel还是per layer量化方案，对于weight权重的量化使用对称量化，对于activate激活的量化使用非对称量化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca5632-0d21-4953-965d-c50c65a62d87",
   "metadata": {},
   "source": [
    "提供了一种量化模式，可以把权重和激活值都量化成8位数字，只保留了一些参数比如bias参数为32位整数  \n",
    "    - bias参数本来不多。  \n",
    "    - bias参数对精度要求高，因为bias-vector会被加到很多激活值上去，所以一个bias值有了误差就会形成全局的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1c77a-d0af-43ae-98c5-947d121d73ca",
   "metadata": {},
   "source": [
    "> Range-Based Linear Quantization  \n",
    "Let's break down the terminology we use here:  \n",
    "Linear: Means a float value is quantized by multiplying with a numeric constant (the scale factor).  \n",
    "Range-Based: Means that in order to calculate the scale factor, we look at the actual range of the tensor's values. In the most naive implementation, we use the actual min/max values of the tensor. ___Alternatively, we use some derivation based on the tensor's range / distribution to come up with a narrower min/max range, in order to remove possible outliers.___ This is in contrast to the other methods described here, which we could call ___clipping-based___, as they impose an explicit clipping function on the tensors (using either a hard-coded value or a learned value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c5e17-4a06-4356-8d5e-67efb886376a",
   "metadata": {},
   "source": [
    "按照量化算法的策略分类大致上可以分为两种，后训练量化和量化感知训练。f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c0df5-3e1d-4465-ad7b-708c2ebdd498",
   "metadata": {},
   "source": [
    "symmetric mm:\n",
    "    $$ q_y = \\frac{s_x*s_w}{s_y}\\sum_{j = 1} ^ {N}(q_x^{i, j}- zp_x)\\ (q_w^{j, k} - zp_w) + zp_y\\tag{1.1}$$\n",
    "    将$\\frac{s_x*s_w}{s_y}$表示为定点：将浮点数表示为一个[0, 1]之间的数和$2^{-n}, \\ \\ \\  n>0$的乘积  \n",
    "    通过计算量化和反量化之后的float值和原始float值之间的均方差可以直观看出，位宽越小误差越大，范围越大误差越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2cf0e-a90c-4689-9faa-64ae1b300e33",
   "metadata": {},
   "source": [
    "The choice of a quantization paradigm affects the calculations that gemmlowp itself needs to perform, specifically, it affects how one goes from internal 32bit accumulator to final 8bit outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71fdbb-8940-467b-98c6-1a0b9715247d",
   "metadata": {},
   "source": [
    "$real_value = scale * (quantized_value - zero_point)        (3)  $  \n",
    "in $\\sum_{j = 1} ^ {N}(q_x^{i, j}- zp_x)\\ (q_w^{j, k} - zp_w) $  \n",
    "Typically, all of these values are uint8. Typically, the above differences of uint8 values would be represented as signed int16; their products as signed int32.  \n",
    "when we don't care about zero point , (1.1) become:\n",
    "$$ yf =  M_0*2^{-n}*uint32\\tag{1.2}$$  \n",
    "to find $ M_0*2^{-n}$ we have $ M = M_0*2^{-n}$\n",
    "so $ M_0 = M * 2 ^ n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84d8a9c-a80f-4db3-8a34-20fa72fe2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1, Mo=0,  error=0.007247\n",
      "n=2, Mo=0,  error=0.007247\n",
      "n=3, Mo=0,  error=0.007247\n",
      "n=4, Mo=0,  error=0.007247\n",
      "n=5, Mo=0,  error=0.007247\n",
      "n=6, Mo=0,  error=0.007247\n",
      "n=7, Mo=1,  error=0.007247\n",
      "n=8, Mo=2,  error=0.007247\n",
      "n=9, Mo=4,  error=0.007247\n",
      "n=10, Mo=7,  error=0.007247\n",
      "n=11, Mo=15,  error=0.007247\n",
      "n=12, Mo=30,  error=0.007247\n",
      "n=13, Mo=59,  error=0.007247\n",
      "n=14, Mo=119,  error=0.007247\n",
      "n=15, Mo=237,  error=0.007247\n"
     ]
    }
   ],
   "source": [
    "\n",
    "M = 0.0072474273418460\n",
    "# P = 7091\n",
    "\n",
    "def multiply(n, M):\n",
    "    # result = M * P\n",
    "    Mo = int(round(2 ** n * M)) # 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理\n",
    "\n",
    "    approx_result = (Mo) >> n\n",
    "    print(\"n=%d, Mo=%d,  error=%f\"%\\\n",
    "          (n, Mo,  M-approx_result))\n",
    "\n",
    "for n in range(1, 16):\n",
    "    multiply(n, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02573843-97ab-40b8-b7cd-217d37f805e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "-----\n",
    "#### 算子量化流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2940b-94e6-472a-b15f-3509d1fc6837",
   "metadata": {},
   "source": [
    "1. 实现反卷积2x2， 和torch核对（数据输入和输出都是channel-last的）\n",
    "2. 实现卷积3x3\n",
    "3. 了解网络结构， 数据size变化，传输方式\n",
    "4. 设计规划对应量化结构，数据类型的转化，量化特征的传输（scale， zero point）\n",
    "5. 实现对应算子的量化流程\n",
    "6. 实现底层的quant, dequant , calc_s_zp\n",
    "7. 以上为非对称实现。拓展一版对称实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a598f-591a-4f32-ac62-0bb53ca1c05e",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### 1. 反卷积2x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5187c-e0e4-4fae-9885-1fa56c6ccdd3",
   "metadata": {},
   "source": [
    "> For an in-depth treatment of the subject, see\n",
    "Chapter 9 of the Deep Learning textbook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232b8b5-9524-4e52-bfcd-009f3a77cced",
   "metadata": {},
   "source": [
    "[___`intro for conv`___](chrome-extension://ikhdkkncnoglghljlkmcimlnlhkeamad/pdf-viewer/web/viewer.html?file=https%3A%2F%2Farxiv.org%2Fpdf%2F1603.07285.pdf)  \n",
    "1. 为什么做卷积  \n",
    "> 通过卷积获得数组维度相关的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0666fe-5bca-45b3-aebd-15f195d81437",
   "metadata": {},
   "source": [
    "\n",
    "卷积次数：\n",
    "$$o = \\lfloor \\frac{i - k + 2p}{s} \\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3986-f336-4435-a070-3a0f4ab3e7f7",
   "metadata": {},
   "source": [
    "如果想卷积后size和卷积前相同， k必须为奇数， s=1， p=round（k/2）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf8f98-fca2-4c3d-b2cb-adebe67a6c98",
   "metadata": {},
   "source": [
    "完全填充：  \n",
    "> every\n",
    "possible partial or complete superimposition of the kernel on the input feature\n",
    "map is taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e652915-990c-4961-a6ed-a4603e4bc31b",
   "metadata": {},
   "source": [
    "- 池化：\n",
    "$$o = \\lfloor \\frac{i - k}{s} \\rfloor + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126cbc1a-513f-4734-a18b-244a0e46dff4",
   "metadata": {},
   "source": [
    "###### trans conv\n",
    "> To maintain the same connectivity pattern in the equivalent convolution it is\n",
    "necessary to zero pad the input in such a way that the first (top-left) application\n",
    "of the kernel only touches the top-left pixel, i.e., the padding has to be equal to\n",
    "the size of the kernel minus one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b03e6-496d-41fc-aabe-4ce2949bcb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
