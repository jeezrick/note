{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0e2332-3905-4ffb-9e4a-dcaca1fd21cc",
   "metadata": {},
   "source": [
    "## goto\n",
    "[论文](https://www.cs.utexas.edu/users/pingali/CS378/2008sp/papers/gotoPaper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead25e9-2e88-4178-bcd3-c8d91bc3e5d7",
   "metadata": {},
   "source": [
    "- The ratio between the rate at which floating point operations (flops) can be\n",
    "performed by the floating point unit(s) and the rate at which floating point\n",
    "numbers can be streamed from the level-2 (L2) cache to registers is typically\n",
    "relatively small. This means that matrix A˜ can be streamed from the L2 cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d2576-ce74-4809-9992-d39ec25da4bd",
   "metadata": {},
   "source": [
    " - All libraries that were timed use assembly-coded inner-kernels (including ATLAS). Compiler\n",
    "options -fomit-frame-pointer -O3 -funroll-all-loops were used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae39d0c-0d5b-4e4c-851a-c2b3754b5c99",
   "metadata": {},
   "source": [
    "- A theory that supports an optimality claim regarding the general approach mentioned in this section can be found in [Gunnels et al. 2001]. In particular, that\n",
    "paper supports the observation that computation should be cast in terms of the\n",
    "decision tree given in Fig. 4 if data movement between memory layers is to be optimally amortized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288192b1-44d0-414d-baaf-cd3f3b7572d9",
   "metadata": {},
   "source": [
    "- However,\n",
    "the bottom line is that under the simplified assumptions A should occupy as much of the cache as possible and should be roughly square2\n",
    ", while leaving room in the\n",
    "cache for at least Bj and Cj ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13974268-7360-4271-9d05-fd3f54ad27ac",
   "metadata": {},
   "source": [
    "- Throughout the remainder of the paper, we will assume\n",
    "that matrices are stored in column-major order.  \n",
    "row major == channel last  \n",
    "column major == channel first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ff606-f9f3-4aa4-9dc7-7e07ed33f436",
   "metadata": {},
   "source": [
    "- This suggests loading matrix A in the cache layer that is farthest from the registers\n",
    "(can hold the most data) subject to the constraint that Assumptions (a)–(c) are  \n",
    "(roughly) met."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0940c1-559b-40be-8a03-f92f21da9d27",
   "metadata": {},
   "source": [
    "cache hit / cache miss  \n",
    "TBL hit / TLB miss\n",
    "- __The most significant difference between a cache miss and a TLB miss is that a\n",
    "cache miss does not necessarily stall the CPU. A small number of cache misses can\n",
    "be tolerated by using algorithmic ___prefetching___ techniques as long as the data can be\n",
    "read fast enough from the memory where it does exist and arrives at the CPU by\n",
    "the time it is needed for computation.__ A TLB miss, by contrast, causes the CPU\n",
    "to stall until the TLB has been updated with the new address. In other words,\n",
    "prefetching can mask a cache miss but not a TLB miss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc6c21-f19c-441c-8c3d-5f8f6f71f071",
   "metadata": {},
   "source": [
    "- The fundamental problem now is that A is typically a submatrix\n",
    "of a larger matrix, and therefore is not contiguous in memory. This in turn means\n",
    "that addressing it requires many more than the minimal number of TLB entries.\n",
    "The solution is to pack A in a contiguous work array, $A^~$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28b02f-02a1-4b01-9bd9-02ac1e119c30",
   "metadata": {},
   "source": [
    "- The reason for the factor two is that when the next blocks of columns Bj+1 and\n",
    "Cj+1 are first addressed, the TLB entries that address them should replace those\n",
    "that were used for Bj and Cj . However, upon completion of Cj := AB˜\n",
    "j + Cj some\n",
    "TLB entries related to A˜ will be the least recently used, and will likely be replaced\n",
    "by those that address Bj+1 and Cj+1. The factor two allows entries related to Bj\n",
    "and Cj to coexist with those for A˜, Bj+1 and Cj+1 and by the time Bj+2 and Cj+2\n",
    "are first addressed, it will be the entries related to Bj and Cj that will be least\n",
    "recently used and therefore replaced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0b577-2b32-4e36-b596-7f837aca1038",
   "metadata": {},
   "source": [
    "这是说， 如果TLB中只有A, Cj， Bj，当Bj+1，Cj+1到来时，某些A的元素就会被认为是使用时间距离最久（ the least recently used）的数据，从而被Bj+1，Cj+1取代。  \n",
    "而如果在TLB中保留A, Cj， Bj，Bj+1，Cj+1。那么 the least recently used就会是Cj， Bj（这应该是必然的）， 那么它们就会取代A的元素被替代，从而不会出现TLB miss。  \n",
    "但是这其中的问题是，TLB的存取策略恒定是LAST USE FIRST OUT吗？真实的存取策略是否比这个假设更为复杂，这个问题需要查更多资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f0186-ff08-4fe2-a96e-37e1154b85e3",
   "metadata": {},
   "source": [
    "- A similar argument can\n",
    "be made for this case. Since the limiting factor is more typically the amount of\n",
    "memory that the TLB can address (e.g., the TLB on a current generation Pentium4\n",
    "can address about 256Kbytes while the L2 cache can hold 2Mbytes), we do not\n",
    "elaborate on the details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572aefde-30b4-4452-99d4-385e4ac368bd",
   "metadata": {},
   "source": [
    "在L2cache限制速度之前， TLB就已经限制速度了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04425d0-b025-4d07-8862-9a8f32c4899f",
   "metadata": {},
   "source": [
    "- The fact that Cj is not contiguous in memory is\n",
    "not much of a concern, since that data is not reused as part of the computation of\n",
    "the gepp operation.\n",
    "这说明我们可以在计算output时就设计store位置，使得计算完成后的output的顺序就是我们想要的顺序， 而不是需要开销很大的重排工作。  \n",
    "对以上涉及到的操作所消耗的时间进行测量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fd810-3153-4c00-9270-ad51129831e2",
   "metadata": {},
   "source": [
    "- Note that a gepdot-based implementation places a block of C in the L2 cache\n",
    "and reads and writes each of its elements as a few columns and rows of A and B\n",
    "are streamed from memory. This requires twice the bandwidth between the L2\n",
    "cache and registers as do the gebp and gepbbased algorithms.  \n",
    "之前并没有考虑过字宽的影响·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f20a8a-88c0-48b1-be5a-e7d3e401d69b",
   "metadata": {},
   "source": [
    " - In practice, the algorithm\n",
    "in Fig. 8 can hide the cost of bringing elements of C from and to memory with\n",
    "computation while it exposes the packing of B as sheer overhead  \n",
    "test this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f1461-02ae-4659-a429-f462e8171e2b",
   "metadata": {},
   "source": [
    "- The algorithm\n",
    "in Fig. 9 can hide the cost of bringing elements of B from memory, but exposes\n",
    "the cost of unpacking C as sheer overhead.   \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f8301-26d4-44e4-8feb-e27fea6de73a",
   "metadata": {},
   "source": [
    " - In addition, the set associativity and cache replacement policy further\n",
    "limit how much of the L1 cache can be occupied by Bj .In practice, kcnr floating\n",
    "point numbers should occupy less than half of the L1 cache so that elements of\n",
    "A˜ and Caux do not evict elements of Bj .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833060f-74e4-4db6-bf2d-6a012868156e",
   "metadata": {},
   "source": [
    "#### hp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e78343-f0c1-408a-9586-8d95e4343c0d",
   "metadata": {},
   "source": [
    "- In further discussion, we will pretend that one can place data in a specific cache and keep it there for the duration of computations. In fact, caches retain data using some cache replacement policy that evicts data that has not been recently used. By carefully ordering computations, we can encourage data to remain in cache, which is what happens in practice.\n",
    "cache repalcement policy 是些什么"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70739a35-ca22-444d-9109-b48524bd4c39",
   "metadata": {},
   "source": [
    "The difference here is that while one can explicitly load registers, the movement of data into caches is merely encouraged by careful ordering of the computation, since replacement of data in cache is handled by the hardware, which has some cache replacement policy similar to \"least recently used\" data gets evicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de437f09-ec8d-4a02-9039-5c3c39cb1601",
   "metadata": {},
   "source": [
    "In practice the movement of the data can often be overlapped with computation (this is known as prefetching). However, clearly only so much can be overlapped, so it is still important to make the ratio favorable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789b5d2-e8da-44cd-acec-fea2f7dfbbee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25ff2790-b321-4093-93dd-4ccbbcd51639",
   "metadata": {},
   "source": [
    "what is this https://github.com/ROCmSoftwarePlatform/MIOpen/issues/1092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f853374-4a5e-4392-abac-e272f1a72239",
   "metadata": {},
   "source": [
    "- The cost of bringing it into the vector registers from some layer in the memory is mostly inconsequential because a lot of computation is performed before it is written back out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76330ab0-4240-41f2-b7ff-a8389fd1fbe2",
   "metadata": {},
   "source": [
    "<img src=\"https://www.cs.utexas.edu/users/flame/laff/pfhp/images/Week3/BLISPicturePack.png\" style=\"zoom:40%\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a9db0-a26c-4177-a663-d33fc821cbf0",
   "metadata": {},
   "source": [
    "- For some computations, there is no opportunity to reuse data to that same extent, which means they are inherently bandwidth bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702bf96-74fe-420a-94fc-57629f392918",
   "metadata": {},
   "source": [
    "- 同样的在level 2 cache里面不动。不停地在level 2 和level 3 的cache之间搬运，从而需要保证本次和下一次的都要在level 2的cache里面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502079e-79d5-479c-964a-99b56487ef38",
   "metadata": {},
   "source": [
    "任务调度策略指计算任务的调度与并行化。在分块 GEMM 算法中，计算负载\n",
    "经过分块被划分成多个计算任务，在多线程环境下，这些计算任务将被调度到不\n",
    "同的线程上并行执行。在 GotoBLAS[29] 与 OpenBLAS[30] 的 GEMM 实现中，计\n",
    "算任务在一个矩阵维度（代码中体现为一个循环）上被并行化，不同线程分配到\n",
    "的计算负载是相等的。由于 kernel 函数（单个计算任务）的数据规模约等于 L2\n",
    "cache 的容量，因此这是一种粗粒度的一维并行化策略。随着单个处理器核数不\n",
    "断增加，参与计算的线程数目随之增加，这种粗粒度的一维并行化在某些情况下\n",
    "（如矩阵形状狭长）性能不佳。因此，BLIS[49] 采用了一种细粒度的多维并行化方\n",
    "法。在 BLIS 库的 GEMM 实现中，kernel 函数的数据规模约等于 L1 cache 的容量，\n",
    "并且支持通过环境变量配置 GEMM 在多个矩阵维度上同时被并行化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63803ce2-376a-4c25-9a68-a7b2892e88db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
